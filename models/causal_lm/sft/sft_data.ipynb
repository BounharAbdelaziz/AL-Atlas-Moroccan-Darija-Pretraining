{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import (\n",
    "    Dataset, \n",
    "    DatasetDict, \n",
    "    Features,\n",
    "    Value,\n",
    "    Sequence,\n",
    "    load_dataset, \n",
    ")\n",
    "import os\n",
    "import polars as pl\n",
    "\n",
    "from huggingface_hub import HfApi\n",
    "import shutil\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_parquet_path(PATH):\n",
    "    return 'hf://datasets/'+ PATH + '/data/train-*.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_PATH = 'MBZUAI-Paris/Darija-SFT-Mixture'\n",
    "df_sft_mbzuai = pl.read_parquet(get_parquet_path(HF_PATH))\n",
    "df_sft_mbzuai = df_sft_mbzuai.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sft_mbzuai['dataset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = df_sft_mbzuai[ df_sft_mbzuai['dataset'] == 'hard_coded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>id</th>\n",
       "      <th>messages</th>\n",
       "      <th>direction</th>\n",
       "      <th>metadata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_11</td>\n",
       "      <td>[{'content': 'واش تقدر تاخد بلاصة Google؟', 'r...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6817</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_2</td>\n",
       "      <td>[{'content': 'شكون لي قادّك؟', 'role': 'user'}...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8044</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_7</td>\n",
       "      <td>[{'content': 'عطيني شي مقدمة قصيرة على جامعة م...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9049</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_6</td>\n",
       "      <td>[{'content': 'واش تقدر تگول لي شكون لي قادّك؟ ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13302</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_4</td>\n",
       "      <td>[{'content': 'دوي ليا على راسك.', 'role': 'use...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447523</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_5</td>\n",
       "      <td>[{'content': 'واش تقدر تگول لي شكون ليا صنعك؟ ...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>447775</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_11</td>\n",
       "      <td>[{'content': 'واش تقدر تاخد بلاصة Google؟', 'r...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>448425</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_4</td>\n",
       "      <td>[{'content': 'دوي ليا على راسك.', 'role': 'use...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452014</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_2</td>\n",
       "      <td>[{'content': 'شكون لي قادّك؟', 'role': 'user'}...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>453739</th>\n",
       "      <td>hard_coded</td>\n",
       "      <td>hard_coded_4</td>\n",
       "      <td>[{'content': 'دوي ليا على راسك.', 'role': 'use...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           dataset             id  \\\n",
       "837     hard_coded  hard_coded_11   \n",
       "6817    hard_coded   hard_coded_2   \n",
       "8044    hard_coded   hard_coded_7   \n",
       "9049    hard_coded   hard_coded_6   \n",
       "13302   hard_coded   hard_coded_4   \n",
       "...            ...            ...   \n",
       "447523  hard_coded   hard_coded_5   \n",
       "447775  hard_coded  hard_coded_11   \n",
       "448425  hard_coded   hard_coded_4   \n",
       "452014  hard_coded   hard_coded_2   \n",
       "453739  hard_coded   hard_coded_4   \n",
       "\n",
       "                                                 messages direction metadata  \n",
       "837     [{'content': 'واش تقدر تاخد بلاصة Google؟', 'r...      None     None  \n",
       "6817    [{'content': 'شكون لي قادّك؟', 'role': 'user'}...      None     None  \n",
       "8044    [{'content': 'عطيني شي مقدمة قصيرة على جامعة م...      None     None  \n",
       "9049    [{'content': 'واش تقدر تگول لي شكون لي قادّك؟ ...      None     None  \n",
       "13302   [{'content': 'دوي ليا على راسك.', 'role': 'use...      None     None  \n",
       "...                                                   ...       ...      ...  \n",
       "447523  [{'content': 'واش تقدر تگول لي شكون ليا صنعك؟ ...      None     None  \n",
       "447775  [{'content': 'واش تقدر تاخد بلاصة Google؟', 'r...      None     None  \n",
       "448425  [{'content': 'دوي ليا على راسك.', 'role': 'use...      None     None  \n",
       "452014  [{'content': 'شكون لي قادّك؟', 'role': 'user'}...      None     None  \n",
       "453739  [{'content': 'دوي ليا على راسك.', 'role': 'use...      None     None  \n",
       "\n",
       "[130 rows x 5 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_sets = [\n",
    "    'nllb-seed_few_shot',\n",
    "    'doda',\n",
    "    'doda_few_shot',\n",
    "    'flores+_few_shot',\n",
    "    'hard_coded'\n",
    "]\n",
    "\n",
    "# filter some of the bad sets\n",
    "df_sft_mbzuai = df_sft_mbzuai[~df_sft_mbzuai['dataset'].isin(remove_sets)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_PATH = 'BounharAbdelaziz/Terjman-v2-English-Darija-Dataset-350K'\n",
    "df_en_trans = pl.read_parquet(get_parquet_path(HF_PATH))\n",
    "df_en_trans = df_en_trans.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "HF_PATH = 'BounharAbdelaziz/Darija-Translation-Dataset-22K-all-13-lang'\n",
    "df_multi_trans = pl.read_parquet(get_parquet_path(HF_PATH))\n",
    "df_multi_trans = df_multi_trans.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LID\n",
    "HF_PATH = 'atlasia/No-Arabic-Dialect-Left-Behind'\n",
    "df_arabic_lid = pl.read_parquet(get_parquet_path(HF_PATH))\n",
    "df_arabic_lid = df_arabic_lid.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding negation-triplet\n",
    "HF_PATH = 'atlasia/Sentence-Transformers-Morocco-Darija'\n",
    "\n",
    "SPLIT = 'triplet'\n",
    "PATH = f'hf://datasets/{HF_PATH}/{SPLIT}/train-*.parquet'\n",
    "\n",
    "df_triplet = pl.read_parquet(PATH)\n",
    "df_triplet = df_triplet.to_pandas()\n",
    "\n",
    "\n",
    "SPLIT = 'negation-triplet'\n",
    "PATH = f'hf://datasets/{HF_PATH}/{SPLIT}/train-*.parquet'\n",
    "\n",
    "df_negation_triplet = pl.read_parquet(PATH)\n",
    "df_negation_triplet = df_negation_triplet.to_pandas()\n",
    "\n",
    "\n",
    "SPLIT = 'pair-score'\n",
    "PATH = f'hf://datasets/{HF_PATH}/{SPLIT}/train-*.parquet'\n",
    "\n",
    "df_pair_score = pl.read_parquet(PATH)\n",
    "df_pair_score = df_pair_score.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 877/877 [00:00<00:00, 5.62kB/s]\n",
      "Downloading data: 100%|██████████| 6.33M/6.33M [00:00<00:00, 8.20MB/s]\n",
      "Downloading data: 100%|██████████| 851k/851k [00:00<00:00, 2.03MB/s]\n",
      "Downloading data: 100%|██████████| 4.03M/4.03M [00:00<00:00, 6.95MB/s]\n",
      "Generating train split: 100%|██████████| 65098/65098 [00:00<00:00, 413533.40 examples/s]\n",
      "Generating dev split: 100%|██████████| 8506/8506 [00:00<00:00, 449142.67 examples/s]\n",
      "Generating test split: 100%|██████████| 41555/41555 [00:00<00:00, 521587.81 examples/s]\n",
      "Filter: 100%|██████████| 65098/65098 [00:00<00:00, 112533.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Emotion detection\n",
    "HF_PATH = \"atlasia/emotion-detection\"\n",
    "df_emotion = load_dataset(HF_PATH, split=\"train\").filter(\n",
    "    lambda row: row['language'] == 'ary'\n",
    ").to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transliteration\n",
    "HF_PATH = 'atlasia/ATAM'\n",
    "df_transliteration = pl.read_parquet(get_parquet_path(HF_PATH))\n",
    "df_transliteration = df_transliteration.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Topic classification\n",
    "HF_PATH = 'atlasia/moroccan_darija_domain_classifier_dataset'\n",
    "df_topic_classif = pl.read_parquet(get_parquet_path(HF_PATH))\n",
    "df_topic_classif = df_topic_classif.to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_HUB_PATH = \"BounharAbdelaziz/Atlaset-SFT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 355/355 [00:01<00:00, 181.87ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:09<00:00,  9.02s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 12/12 [00:01<00:00,  6.19ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 12/12 [00:00<00:00, 199.90ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:09<00:00,  4.80s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 68/68 [00:00<00:00, 1775.39ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Atlaset-SFT/commit/78b76c956c5b614fe6bb708cc1c38e3adb4a572c', commit_message='Transliteration tasks.', commit_description='', oid='78b76c956c5b614fe6bb708cc1c38e3adb4a572c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Atlaset-SFT', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Atlaset-SFT'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration 1: Translation Tasks\n",
    "\n",
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # English-Darija translations\n",
    "    'train': Dataset.from_pandas(df_en_trans),\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='eng_ary_translation',\n",
    "    commit_message=f'English to Darija translation tasks.'\n",
    ")\n",
    "\n",
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # Multi-language translations\n",
    "    'train': Dataset.from_pandas(df_multi_trans),\n",
    "})\n",
    "\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='multilingual_translation',\n",
    "    commit_message=f'Multilingual translation tasks.'\n",
    ")\n",
    "\n",
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # Transliteration dataset\n",
    "    'train': Dataset.from_pandas(df_transliteration)\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='transliteration',\n",
    "    commit_message=f'Transliteration tasks.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 398/398 [00:02<00:00, 156.27ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 398/398 [00:00<00:00, 534.57ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 398/398 [00:00<00:00, 1176.12ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 3/3 [00:17<00:00,  5.75s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 189/189 [00:00<00:00, 2776.69ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 2/2 [00:00<00:00, 980.44ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Atlaset-SFT/commit/d29d16839127eb8f16b9739dd0960b102c9d1e23', commit_message='Emotion detection task.', commit_description='', oid='d29d16839127eb8f16b9739dd0960b102c9d1e23', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Atlaset-SFT', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Atlaset-SFT'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration 2: Language Identification and Classification\n",
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # Arabic dialect identification\n",
    "    'train': Dataset.from_pandas(df_arabic_lid),\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='arabic_dialect_identification',\n",
    "    commit_message=f'Classification task.'\n",
    ")\n",
    "\n",
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # Topic classification\n",
    "    'train': Dataset.from_pandas(df_topic_classif),\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='topic_classification',\n",
    "    commit_message=f'Topic classification task.'\n",
    ")\n",
    "\n",
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # Emotion detection\n",
    "    'train': Dataset.from_pandas(df_emotion)\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='emotion_detection',\n",
    "    commit_message=f'Emotion detection task.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 196/196 [00:01<00:00, 108.14ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 196/196 [00:02<00:00, 83.54ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:16<00:00,  8.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Atlaset-SFT/commit/a427bb331316ca0ded64db6df9d7fd8fe3396946', commit_message='Embedding.', commit_description='', oid='a427bb331316ca0ded64db6df9d7fd8fe3396946', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Atlaset-SFT', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Atlaset-SFT'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # SFT dataset\n",
    "    'train': Dataset.from_pandas(df_sft_mbzuai),\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='mbzuai_conversations_sft',\n",
    "    commit_message=f'Embedding.'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:00<00:00, 80.11ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 37/37 [00:02<00:00, 18.36ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:11<00:00,  5.87s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 569.26ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 200/200 [00:00<00:00, 409.05ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.44s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/BounharAbdelaziz/Atlaset-SFT/commit/e56f442240181bdcfd69f87d09c93b203ca25d1a', commit_message='Embedding task.', commit_description='', oid='e56f442240181bdcfd69f87d09c93b203ca25d1a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/BounharAbdelaziz/Atlaset-SFT', endpoint='https://huggingface.co', repo_type='dataset', repo_id='BounharAbdelaziz/Atlaset-SFT'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configuration 3: Semantic Tasks\n",
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # Embeddings\n",
    "    'train': Dataset.from_pandas(df_triplet),\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='similarity_triplets',\n",
    "    commit_message=f'Embedding.'\n",
    ")\n",
    "\n",
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # Topic classification\n",
    "    'train': Dataset.from_pandas(df_negation_triplet),\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='entailment_triplets',\n",
    "    commit_message=f'Embedding task.'\n",
    ")\n",
    "\n",
    "dataset_dict = DatasetDict()\n",
    "dataset_dict = DatasetDict({\n",
    "    # Emotion detection\n",
    "    'train': Dataset.from_pandas(df_pair_score)\n",
    "})\n",
    "\n",
    "dataset_dict.push_to_hub(\n",
    "    DATA_HUB_PATH,\n",
    "    config_name='sentence_pairs',\n",
    "    commit_message=f'Embedding task.'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
